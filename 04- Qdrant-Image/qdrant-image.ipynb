{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `Image Search with Pinecone and ConvBase for Feature Extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import timm    ## PyTorch Image Models (timm)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, Batch, PointIdsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dotenv file\n",
    "_ = load_dotenv(override=True)\n",
    "qdrant_key = os.getenv('QDRANT_API_KEY')\n",
    "qdrant_url = os.getenv('QDRANT_URL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paths</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset-images\\0009fc27d9.jpg</td>\n",
       "      <td>3054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset-images\\0014c2d720.jpg</td>\n",
       "      <td>3055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset-images\\00196e8fac.jpg</td>\n",
       "      <td>3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset-images\\001fc748e6.jpg</td>\n",
       "      <td>3057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset-images\\002bb8e03b.jpg</td>\n",
       "      <td>3058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>dataset-images\\16af889d9d.jpg</td>\n",
       "      <td>3549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>dataset-images\\16b44ef03b.jpg</td>\n",
       "      <td>3550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>dataset-images\\16b501e949.jpg</td>\n",
       "      <td>3551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>dataset-images\\16bbc4b4dc.jpg</td>\n",
       "      <td>3552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>dataset-images\\16c040e85f.jpg</td>\n",
       "      <td>3553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             paths    id\n",
       "0    dataset-images\\0009fc27d9.jpg  3054\n",
       "1    dataset-images\\0014c2d720.jpg  3055\n",
       "2    dataset-images\\00196e8fac.jpg  3056\n",
       "3    dataset-images\\001fc748e6.jpg  3057\n",
       "4    dataset-images\\002bb8e03b.jpg  3058\n",
       "..                             ...   ...\n",
       "495  dataset-images\\16af889d9d.jpg  3549\n",
       "496  dataset-images\\16b44ef03b.jpg  3550\n",
       "497  dataset-images\\16b501e949.jpg  3551\n",
       "498  dataset-images\\16bbc4b4dc.jpg  3552\n",
       "499  dataset-images\\16c040e85f.jpg  3553\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the images paths\n",
    "imgaes_paths = [os.path.join('dataset-images', img) for img in os.listdir('dataset-images')]\n",
    "\n",
    "## Create a DF contains the images_paths and create a random ID\n",
    "df = pd.DataFrame({'paths': imgaes_paths})\n",
    "df['id'] = np.arange(3054, 3054+len(df), 1)\n",
    "\n",
    "## Take only the first 500 images --> for simplicity\n",
    "df_use = df.iloc[:500]\n",
    "df_use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `ConvBase for Feature Extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here, I will use VGG19 Model ConvBase using timm library as a convbase for feature extraction\n",
    "## The VGG19 Model after flattening the vector it will be of lenght 4096.\n",
    "\n",
    "model = timm.create_model('vgg19', pretrained=True)\n",
    "model = nn.Sequential(*list(model.children())[:-1])\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Lenght using VGG19 Model is: 4096\n"
     ]
    }
   ],
   "source": [
    "def extract_images_features(images_paths: list):\n",
    "    ''' This Function is taking a list of images_paths and returns the features extraction from them using VGG19 Model.\n",
    "    '''\n",
    "\n",
    "    ## Transformation before extraction\n",
    "    transform = transforms.Compose([   \n",
    "                            ## VGG required images (224, 224)\n",
    "                            transforms.Resize((224, 224)),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                                   ])\n",
    "    \n",
    "    # Looping over the images_paths\n",
    "    batch_features = []\n",
    "    for image_path in images_paths:\n",
    "        ## Convert it to Pillow and then to tensor\n",
    "        image_tensor = Image.open(image_path).convert('RGB')\n",
    "        image_tensor = transform(image_tensor).unsqueeze(0)\n",
    "\n",
    "        ## Pass the Image and get the Feature Extraction\n",
    "        with torch.no_grad():\n",
    "            conv_features = model(image_tensor)\n",
    "            ## Flatten --> I want a vector as a list in 1D\n",
    "            image_features = conv_features.view(conv_features.size(0), -1).tolist()[0]\n",
    "\n",
    "        ## Append to the list\n",
    "        batch_features.append(image_features)\n",
    "\n",
    "    return batch_features\n",
    "\n",
    "## Test the above function\n",
    "vgg19_vect_length = len(extract_images_features(images_paths=[r'dataset-images\\0009fc27d9.jpg'])[0])\n",
    "print(f'Vector Lenght using VGG19 Model is: {vgg19_vect_length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Upserting to Qdrant`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Connect to Qdrant Client\n",
    "\n",
    "## Initilaize a Client\n",
    "client = QdrantClient(url=qdrant_url, api_key=qdrant_key)\n",
    "\n",
    "## Collection Configurations\n",
    "collection_config = VectorParams(\n",
    "                            size=4096,                    ## The lenght of vgg19 convabase model\n",
    "                            distance=Distance.COSINE,     ## The similarity metric\n",
    "                            on_disk=True                  ## RAM optimizing\n",
    "                                )\n",
    "\n",
    "## Create a Collection \n",
    "client.recreate_collection(collection_name='image-search-course', vectors_config=collection_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is: green\n",
      "Vectors Count is: 0\n"
     ]
    }
   ],
   "source": [
    "## Check Status of Collection\n",
    "collection_status = client.get_collection(collection_name='image-search-course').status\n",
    "collection_count_vectors = client.get_collection(collection_name='image-search-course').vectors_count\n",
    "\n",
    "print(f'Status is: {collection_status}')\n",
    "print(f'Vectors Count is: {collection_count_vectors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [05:39<00:00, 21.24s/it]\n"
     ]
    }
   ],
   "source": [
    "## Function for upserting data to Qdrant\n",
    "\n",
    "def upsert_to_qdrant(df, batch_size=32):\n",
    "\n",
    "    ## A list for failed_ids\n",
    "    failed_ids = []\n",
    "\n",
    "    for batch_start in tqdm(range(0, len(df), batch_size)):\n",
    "\n",
    "        try:\n",
    "            ## Prepare batches\n",
    "            batch_end = min(batch_start+batch_size, len(df))\n",
    "            paths_batch = df['paths'][batch_start: batch_end].tolist()\n",
    "            ids_batch = df['id'][batch_start: batch_end].tolist()     ## No need to be converted to string (Qdrant need integer)\n",
    "\n",
    "            ## Extract Features by calling the function\n",
    "            batch_features = extract_images_features(images_paths=paths_batch)\n",
    "\n",
    "            ## Prepare to Qdrant\n",
    "            to_upsert = Batch(ids=ids_batch, vectors=batch_features)\n",
    "\n",
    "            ## Upsert to Qdrant\n",
    "            client.upsert(collection_name='image-search-course', wait=True, points=to_upsert)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error in upserting: {e}')\n",
    "            failed_ids.append(ids_batch)\n",
    "\n",
    "    return failed_ids\n",
    "\n",
    "\n",
    "## Apply the function\n",
    "failed_ids = upsert_to_qdrant(df=df_use, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is: green\n",
      "Vectors Count is: 500\n"
     ]
    }
   ],
   "source": [
    "## Check Status of Collection after upserting\n",
    "collection_status = client.get_collection(collection_name='image-search-course').status\n",
    "collection_count_vectors = client.get_collection(collection_name='image-search-course').vectors_count\n",
    "\n",
    "print(f'Status is: {collection_status}')\n",
    "print(f'Vectors Count is: {collection_count_vectors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id=3263, version=6, score=0.7062105, payload={}, vector=None),\n",
       " ScoredPoint(id=3439, version=12, score=0.55330193, payload={}, vector=None),\n",
       " ScoredPoint(id=3080, version=0, score=0.4998963, payload={}, vector=None),\n",
       " ScoredPoint(id=3491, version=13, score=0.4963698, payload={}, vector=None),\n",
       " ScoredPoint(id=3124, version=2, score=0.46910954, payload={}, vector=None),\n",
       " ScoredPoint(id=3167, version=3, score=0.4662863, payload={}, vector=None),\n",
       " ScoredPoint(id=3204, version=4, score=0.41910368, payload={}, vector=None),\n",
       " ScoredPoint(id=3527, version=14, score=0.41092783, payload={}, vector=None),\n",
       " ScoredPoint(id=3177, version=3, score=0.4023589, payload={}, vector=None)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Inference in real-time\n",
    "image_new_path = df['paths'].iloc[-1]\n",
    "image_feats_new = extract_images_features(images_paths=[image_new_path])[0]\n",
    "\n",
    "client.search(collection_name='image-search-course', query_vector=image_feats_new, limit=100, score_threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=16, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## delete example using id\n",
    "# client.delete(collection_name='image-search-course', points_selector=PointIdsList(points=[3054]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vectdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
